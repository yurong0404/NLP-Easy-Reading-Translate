{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/YuRong/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_API_URI = \"https://{0}.linggle.com/query/\"\n",
    "class Linggle:\n",
    "    def __init__(self, ver='www'):\n",
    "        self.ver = ver\n",
    "\n",
    "    def __getitem__(self, query):\n",
    "        return self.search(query)\n",
    "\n",
    "    def search(self, query):\n",
    "        query = query.replace('/', '@')\n",
    "        req = requests.get(NGRAM_API_URI.format(self.ver) + query)\n",
    "        results = req.json()\n",
    "        return results.get('ngrams', [])\n",
    "    \n",
    "    \n",
    "def extract(soup):\n",
    "    word_list = []\n",
    "    for term in soup.select('.pt-list-terms'):\n",
    "        count = 0    \n",
    "        for item in term.select('.pt-list-terms__item'):\n",
    "            if item.select('.pt-list-rating__indicator--high'):\n",
    "                for title in item.select('.pt-thesaurus-card__term-title'):\n",
    "                    if title.select_one('.link--term'):\n",
    "                        if count == 3:\n",
    "                            break\n",
    "                        else:\n",
    "                            count += 1\n",
    "                            word_list.append(title.select_one('.link--term').text)\n",
    "    return word_list\n",
    "\n",
    "def crawl(url):\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0' }\n",
    "    source_code = requests.get(url , headers=headers).content\n",
    "    soup = BeautifulSoup(source_code, 'html.parser')\n",
    "    return extract(soup)\n",
    "    \n",
    "def synonymsPT(word):\n",
    "    r = 'https://www.powerthesaurus.org/'+ word +'/synonyms'\n",
    "    synonymsList = crawl(r)\n",
    "    max_count = 0\n",
    "    max_word = word\n",
    "    for synonym in synonymsList:\n",
    "        count = sum([row[1] for row in ling[word+\" _\"]][:10]) + sum([row[1] for row in ling[\"_ \"+word]][:10])\n",
    "        s_count = sum([row[1] for row in ling[synonym+\" _\"]][:10]) + sum([row[1] for row in ling[\"_ \"+synonym]][:10])\n",
    "        if s_count > (2*count) and s_count > 800000:\n",
    "            max_count = s_count\n",
    "            max_word = synonym\n",
    "    return max_word\n",
    "\n",
    "def crawlCNN(url):\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0' }\n",
    "    source_code = requests.get(url , headers=headers).content\n",
    "    soup = BeautifulSoup(source_code, 'html.parser')\n",
    "    \n",
    "    paragraph = \"\"\n",
    "    for big_block in soup.select('.zn-body__paragraph'):\n",
    "        paragraph += big_block.text + '\\n'\n",
    "    \n",
    "    return paragraph\n",
    "\n",
    "def modifiedParagraph(origin, diffWord):\n",
    "    origin = origin.split(' ')\n",
    "    for word in diffWord:\n",
    "        origin[word[0]] = origin[word[0]].replace(word[2], word[4])\n",
    "    modified = ' '.join(origin)\n",
    "    simpleWordList = [x[4] for x in diffWord]\n",
    "    simpleWord = ' '.join(simpleWordList)\n",
    "    return modified, simpleWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read glove embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(os.getcwd()+\"/glove_300d_word2vec.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read English 5000 vocabulary dictionary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Oxford 5000 vocabulary from file\n",
    "f = open('5000_voc_Oxford.txt','r')\n",
    "words_5000 = []\n",
    "for line in f.readlines():\n",
    "    line = line.lower()\n",
    "    line = line.strip('\\n')\n",
    "    line = line.split('\\t',1)\n",
    "    words_5000.append([line[0]])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input paragraph file (Optional, or you can use CNN url to read paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('input.txt','r')\n",
    "origin = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input CNN paragraph URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_URL = 'https://edition.cnn.com/2019/06/15/asia/hong-kong-extradition-law-china-intl-hnk/index.html'\n",
    "origin = crawlCNN(CNN_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin paragraph:\n",
      "Hong Kong (CNN)Hong Kong's government has blinked. The city's leader Carrie Lam announced Saturday she was suspending a controversial extradition bill after mass protests and sustained opposition from multiple sectors of society. \n",
      "While she fell short of fully withdrawing the bill -- or resigning -- as some protesters had demand, the move is nevertheless a major win for the hundreds of thousands who took to the streets on June 9, as well as the tens of thousands of mostly young protesters who shut down parts of the city on Wednesday and prevented lawmakers from beginning a second reading. \n",
      "Protest organizers are calling for another mass rally on Sunday in a bid to keep the pressure on the government. Protesters will wear black and demand the bill be completely withdrawn and Lam step down. \n",
      "Pro-democracy figures claim the bill's reintroduction would lead to the erosion of civil rights in Hong Kong, including freedom of speech and rule of law, and could see residents sent to China to face prosecution in a country with an opaque legal system.\n",
      "Beijing has yet to officially react to Lam's announcement that she was suspending the bill, though the Chief Executive has emphasized that she had the central government's support. \n",
      "From all available accounts, it appears that the bill was an initiative of Lam's government -- as she has claimed all along -- rather than an order from on high. \n",
      "Lam and her ministers appear to have seen in a gruesome Taiwan murder case a way to win an easy public relations victory by extraditing a wanted killer to face justice, and a way to close loopholes and extend the central government's powers to go after fugitives, especially former Chinese officials, in Hong Kong. \n",
      "They do not, however, seem to have expected the overwhelmingly negative response to the bill and the deep, widespread distrust for the Chinese legal system. Pro-democracy activists, NGOs and business groups came together in calling for the bill's withdrawal, and it also served to unify the previously fractured political opposition. \n",
      "Protests this month -- with the June 9 march the largest since Hong Kong was handed over to Chinese control, and Wednesday's protests among some of the most violent scenes ever seen in the city -- left Lam unwilling to push forward with a bill so clearly unpopular and potentially dangerous. \n",
      "However, while the climbdown is certainly embarrassing to both the Hong Kong and Beijing governments, it only maintains the status quo, as did a mass movement against an anti-sedition law in 2003, previously the largest protests Hong Kong had seen under Chinese rule. \n",
      "Hong Kong residents have proven more willing to come to the streets to fight back against a loss of political freedoms than to push for extra ones, and both the local and central governments appear to have a greater degree of flexibility or patience on these issues as well. \n",
      "The situation in 2014, when demands for the direct election of the chief executive spiraled into the Umbrella Movement, was very different. Those protests did not attract such a broad swath of society, and they were also greeted with a much more forceful reaction by Beijing, and a subsequent crackdown and numerous prosecutions and disqualifications of lawmakers. \n",
      "Does the bill have a future?\n",
      "While Lam emphasized that the bill has been suspended rather than completely withdrawn, it is likely that the effect will be the same, at least in the near term. \n",
      "As opposition to the bill grew, Taipei said it would not request the murder suspect's extradition from Hong Kong, as it said the bill would put Taiwanese citizens in danger. With this off the table, Lam admitted there was \"less urgency to pass the bill this year.\" \n",
      "Going forward, she said she wanted to focus on \"economic and livelihood\" issues, particularly those such as housing, a consistent major issue in Hong Kong, particularly for young people of the sort out protesting Wednesday. \n",
      "That language is similar to what she has used in the past when discussing whether her government would seek to introduce Article 23, the anti-sedition law which was shelved after mass protests against it in 2003. Lam has said she would only do so were the conditions in society correct, and on Saturday she said she regretted that controversies over the extradition bill had spoiled the \"period of calm\" Hong Kong had enjoyed since she took office in 2017. \n",
      "While the government could reintroduce the bill next year, it is unlikely as 2020 is an election year for the legislative council.\n",
      "The newly reinvigorated and unified pro-democratic camp will be targeting marginal seats in an attempt to wrest back veto power in the legislature, and pro-Beijing lawmakers have already warned that the controversies over the bill could cost them seats. \n",
      "So though some protesters and opposition figures may complain that Lam has only suspended rather than withdrawn the bill, the effect may end up being the same. \n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Origin paragraph:')\n",
    "print(origin,'\\n')\n",
    "# paragraph is a list of origin paragraph split by ' '\n",
    "paragraph = origin.split(' ')\n",
    "\n",
    "#token_pos => [('Tokyo', 'NNP'), ('(', '('), ('CNN', 'NNP'), (')', ')'), ('Japan', 'NNP'), (\"'s\", 'POS'), ....]\n",
    "token_pos = nltk.pos_tag(nltk.word_tokenize(origin))\n",
    "\n",
    "#word_token => dictionary with key = paragraph's index, value = word_tokenize(value of paragraph)\n",
    "#word_token => {0: ['Tokyo'], 1:['(', 'CNN', ')', 'Japan', \"'s\"], 2:['85-year-old'], ....}\n",
    "word_token = {}\n",
    "for index, token in enumerate(paragraph):\n",
    "    word_token[index] = nltk.word_tokenize(token)\n",
    "\n",
    "#list of (word_token's index, pos, original token, origina token's lower and simple tense || original token)\n",
    "#allword => [(0, 'NNP', 'Tokyo', 'Tokyo'), (1, '(', '(', '('), (1, 'NNP', 'CNN', 'CNN'), .....]\n",
    "allword = []\n",
    "verb_pos = ['VBD','VBG','VBN','VBP','VBZ']\n",
    "noun_pos = ['NNS']\n",
    "posIndex = 0\n",
    "for key, value in word_token.items():\n",
    "    for v in value:\n",
    "        if posIndex >= len(token_pos):\n",
    "            break\n",
    "        current_pos = token_pos[posIndex][1]\n",
    "        #simplify the word tense of V. and N.\n",
    "        if current_pos in verb_pos:\n",
    "            word = (key, current_pos, v, WordNetLemmatizer().lemmatize(v.lower(),'v'))\n",
    "            allword.append(word) \n",
    "        elif current_pos in noun_pos:\n",
    "            word = (key, current_pos, v, WordNetLemmatizer().lemmatize(v.lower(),'n'))\n",
    "            allword.append(word)\n",
    "        else:\n",
    "            word = (key, current_pos, v, v)\n",
    "            allword.append(word)\n",
    "        posIndex += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find difficult words and find the simpler word to replace it.\n",
    "## All difficult words and the simpler words are stored in variable \"diffWord \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "ling = Linggle()\n",
    "#the POS we check whether the word is difficult.\n",
    "care_pos = ['NN', 'NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP',' VBZ']\n",
    "#print('Difficult words:')\n",
    "diffWord = []\n",
    "dont_care_word = ['``', ',', '.', \"'d\", \"'s\", \"''\"]\n",
    "for word in allword:\n",
    "    if word[2] in dont_care_word or word[3] in dont_care_word:\n",
    "        continue\n",
    "    if word[3] not in [i[0] for i in words_5000] and word[3].lower() not in [i[0] for i in words_5000] and word[1] in care_pos:\n",
    "        try:\n",
    "            count = sum([row[1] for row in ling[word[2]+\" _\"]][:10]) + sum([row[1] for row in ling[\"_ \"+word[2]]][:10])\n",
    "        except:\n",
    "            continue\n",
    "        #如果字在linggle的次數大於150萬，就篩掉，不視為困難字\n",
    "        if count < 1500000:\n",
    "            #==================== 用word embedding換同義字 ==========================\n",
    "            #similar word => 用word embedding前十相近的字挑出分數大於0.6的\n",
    "            try:\n",
    "                similar_word = [row[0] for row in model.most_similar(word[2], topn=10) if row[1] > 0.6]\n",
    "                max_count = 0\n",
    "                max_word = word[2]\n",
    "                for s_word in similar_word:\n",
    "                    s_count = sum([row[1] for row in ling[s_word+\" _\"]][:10]) + sum([row[1] for row in ling[\"_ \"+s_word]][:10])\n",
    "                    #從similar word挑出在linggle次數最多的，且次數必須大於原字的2倍，且次數要大於120萬\n",
    "                    if s_count > (2*count) and s_count > max_count and s_count > 1200000:\n",
    "                        max_count = s_count\n",
    "                        max_word = s_word\n",
    "                if word[2] != max_word:\n",
    "                    word = word+(max_word, 'word embedding')\n",
    "                    diffWord.append(word)\n",
    "                #=============== 如果沒利用word embedding換字，則使用power thesaurus網佔找同義字 =================\n",
    "                else:\n",
    "                    replace_word = synonymsPT(word[2])\n",
    "                    if replace_word == word[2] or replace_word == word[3]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        word = word + (replace_word, 'PT website')\n",
    "                        diffWord.append(word)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19, 'NN', 'extradition', 'extradition', 'surrender', 'PT website'),\n",
       " (92, 'NNS', 'lawmakers', 'lawmaker', 'legislation', 'word embedding'),\n",
       " (269, 'NNS', 'loopholes', 'loophole', 'shortcomings', 'PT website'),\n",
       " (306, 'NN', 'distrust', 'distrust', 'dislike', 'word embedding'),\n",
       " (334, 'VBN', 'fractured', 'fracture', 'broken', 'word embedding'),\n",
       " (510, 'NN', 'swath', 'swath', 'path', 'PT website'),\n",
       " (529, 'NN', 'crackdown', 'crackdown', 'protests', 'word embedding'),\n",
       " (536, 'NNS', 'lawmakers', 'lawmaker', 'legislation', 'word embedding'),\n",
       " (586, 'NN', 'extradition', 'extradition', 'surrender', 'PT website'),\n",
       " (629, 'NN', 'livelihood', 'livelihood', 'support', 'PT website'),\n",
       " (651, 'NN', 'protesting', 'protesting', 'protest', 'word embedding'),\n",
       " (681, 'VBN', 'shelved', 'shelve', 'filed', 'PT website'),\n",
       " (714, 'NN', 'extradition', 'extradition', 'surrender', 'PT website'),\n",
       " (736, 'VB', 'reintroduce', 'reintroduce', 'restore', 'PT website'),\n",
       " (755, 'VBN', 'reinvigorated', 'reinvigorate', 'fresh', 'PT website'),\n",
       " (769, 'VB', 'wrest', 'wrest', 'extract', 'PT website'),\n",
       " (771, 'NN', 'veto', 'veto', 'override', 'word embedding'),\n",
       " (778, 'NNS', 'lawmakers', 'lawmaker', 'legislation', 'word embedding')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
